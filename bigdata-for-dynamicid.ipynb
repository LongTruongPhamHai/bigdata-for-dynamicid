{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14255569,"sourceType":"datasetVersion","datasetId":9029656}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BIGDATA FOR DYNAMICID","metadata":{}},{"cell_type":"markdown","source":"## Clone DynamicID Repository\n- G·ªëc: https://github.com/traananhdat/DynamicID\n- M·ªõi: https://github.com/LongTruongPhamHai/bigdata-for-dynamicid","metadata":{}},{"cell_type":"code","source":"# X√≥a th∆∞ m·ª•c c≈© n·∫øu ƒë√£ t·ªìn t·∫°i\n!rm -rf /kaggle/working/bigdata-for-dynamicid\n\n# Clone repo github\n!git clone https://github.com/LongTruongPhamHai/bigdata-for-dynamicid","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train SAA","metadata":{}},{"cell_type":"code","source":"# T·∫£i th∆∞ vi·ªán\n!pip uninstall -y diffusers huggingface_hub transformers accelerate peft\n\n!pip install \\\n  diffusers==0.27.2 \\\n  transformers==4.41.2 \\\n  accelerate==0.27.2 \\\n  peft==0.10.0 \\\n  huggingface_hub==0.23.4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ki·ªÉm tra th∆∞ vi·ªán\n!pip show diffusers transformers accelerate peft","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch \\\n  --num_processes 1 \\\n  /kaggle/working/bigdata-for-dynamicid/train_SAA.py \\\n  --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 \\\n  --image_encoder_path openai/clip-vit-large-patch14 \\\n  --data_root_path /kaggle/input/dataset-for-dynamicid/dataset/base_image_dataset \\\n  --data_json_file /kaggle/working/bigdata-for-dynamicid/data.json \\\n  --mixed_precision fp16 \\\n  --train_batch_size 1 \\\n  --num_train_epochs 10","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# N√©n file checkpoint\n!rm /kaggle/working/checkpoint-2000.zip\n# !zip -r /kaggle/working/checkpoint-2000.zip /kaggle/working/sd-ip_adapter/checkpoint-2000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T15:23:41.098983Z","iopub.execute_input":"2025-12-22T15:23:41.099842Z","iopub.status.idle":"2025-12-22T15:23:41.216915Z","shell.execute_reply.started":"2025-12-22T15:23:41.099781Z","shell.execute_reply":"2025-12-22T15:23:41.216156Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# X√≥a 3 file checkpoint kh√¥ng c·∫ßn thi·∫øt\n!rm /kaggle/working/sd-ip_adapter/checkpoint-2000/optimizer.bin \\\n    /kaggle/working/sd-ip_adapter/checkpoint-2000/random_states_0.pkl \\\n    /kaggle/working/sd-ip_adapter/checkpoint-2000/scaler.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T15:25:27.794017Z","iopub.execute_input":"2025-12-22T15:25:27.794698Z","iopub.status.idle":"2025-12-22T15:25:27.798264Z","shell.execute_reply.started":"2025-12-22T15:25:27.794660Z","shell.execute_reply":"2025-12-22T15:25:27.797604Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Train IMR","metadata":{}},{"cell_type":"markdown","source":"### T·∫°o SAA.bin t·ª´ /sd-ip_adapter/checkpoint-2000/model.safetensors","metadata":{}},{"cell_type":"code","source":"import torch\nfrom safetensors.torch import load_file\n\nckpt_path = \"sd-ip_adapter/checkpoint-2000/model.safetensors\"\nout_path = \"models/SAA.bin\"\n\nstate_dict = load_file(ckpt_path)\n\n# ch·ªâ gi·ªØ c√°c key thu·ªôc adapter / SAA\nadapter_state = {\n    k: v for k, v in state_dict.items()\n    if \"adapter\" in k.lower() or \"lora\" in k.lower()\n}\n\ntorch.save(adapter_state, out_path)\nprint(\"Exported SAA.bin\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell c≈© - Ch·ªâ xem, kh√¥ng c·∫ßn ch·∫°y","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocess\n### Rename image (Ch·∫°y tr·ª±c ti·∫øp tr√™n m√°y)","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\n\ndef rename_images(directory: Path):\n    files = [f for f in directory.iterdir() if f.is_file()]\n    files.sort()  # s·∫Øp x·∫øp ƒë·ªÉ c√≥ th·ª© t·ª± ·ªïn ƒë·ªãnh\n\n    changed = 0\n    for idx, f in enumerate(files, start=1):\n        if idx > 35:\n            break\n\n        new_name = f\"{idx:04d}.jpg\"\n        new_path = f.with_name(new_name)\n\n        if new_path == f:\n            continue  # ƒë√£ ƒë√∫ng ƒë·ªãnh d·∫°ng\n        if new_path.exists():\n            print(f\"SKIP: T·ªìn t·∫°i s·∫µn {new_path.name}, b·ªè qua {f.name}\")\n            continue\n\n        f.rename(new_path)\n        changed += 1\n        print(f\"RENAMED: {f.name} -> {new_path.name}\")\n\n    print(f\"Done. ƒê√£ ƒë·ªïi t√™n {changed} file.\")\n\n\nif __name__ == \"__main__\":\n    dir_path = Path(\n        r\"D:\\DEV\\.Projects\\TLU\\PTDL\\DynamicID_fork_prv\\dataset\\base_image_dataset\\2\"\n    ).resolve()\n    rename_images(dir_path)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### FaceCrop Image (ƒê√£ x·ª≠ l√Ω v√† l∆∞u v√†o dataset-for-dynamicid)\nModel: MediaPipe Face Detection","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y numpy\n!pip install numpy==1.26.4 mediapipe==0.10.31 opencv-python","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mediapipe==0.10.31 opencv-python","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X√≥a th∆∞ m·ª•c n·∫øu ƒë√£ t·ªìn t·∫°i\n!rm -rf /kaggle/working/mediapipe_models\n\n# T·∫°o th∆∞ m·ª•c m·ªõi\n!mkdir -p /kaggle/working/mediapipe_models\n\n# T·∫£i model short-range\n!wget \\\nhttps://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/latest/blaze_face_short_range.tflite \\\n-O /kaggle/working/mediapipe_models/blaze_face_short_range.tflite","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\nfrom mediapipe import Image, ImageFormat\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\n\n# ===== PATH =====\nbase_input_dir = \"/kaggle/input/dataset-for-dynamicid/dataset/base_image_dataset\"\nbase_output_dir = \"/kaggle/working/dataset/base_image_dataset\"\nshort_model = \"/kaggle/working/mediapipe_models/blaze_face_short_range.tflite\"\n\ndef load_detector(model_path):\n    base_options = python.BaseOptions(model_asset_path=model_path)\n    options = vision.FaceDetectorOptions(\n        base_options=base_options,\n        running_mode=vision.RunningMode.IMAGE\n    )\n    return vision.FaceDetector.create_from_options(options)\n\n# ===== LOAD MODEL =====\ndetector = load_detector(short_model)\n\n# ===== PROCESS ALL SUBFOLDERS =====\nfor subfolder in sorted(os.listdir(base_input_dir)):\n    input_dir = os.path.join(base_input_dir, subfolder)\n    output_dir = os.path.join(base_output_dir, subfolder)\n\n    if not os.path.isdir(input_dir):\n        continue  # b·ªè qua n·∫øu kh√¥ng ph·∫£i th∆∞ m·ª•c\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    print(f\"üîÑ Processing folder: {subfolder}\")\n\n    for file in os.listdir(input_dir):\n        if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n            continue\n\n        img = cv2.imread(os.path.join(input_dir, file))\n        if img is None:\n            continue\n\n        h, w, _ = img.shape\n        mp_image = Image(\n            image_format=ImageFormat.SRGB,\n            data=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        )\n\n        result = detector.detect(mp_image)\n\n        if not result.detections:\n            # ‚ùå Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c m·∫∑t ‚Üí l∆∞u nguy√™n ·∫£nh\n            cv2.imwrite(os.path.join(output_dir, file), img)\n            print(f\"‚ö†Ô∏è No face detected, saved original: {file}\")\n            continue\n\n        # l·∫•y khu√¥n m·∫∑t ƒë·∫ßu ti√™n\n        bbox = result.detections[0].bounding_box\n        x1, y1 = int(bbox.origin_x), int(bbox.origin_y)\n        bw, bh = int(bbox.width), int(bbox.height)\n        x2, y2 = x1 + bw, y1 + bh\n\n        # m·ªü r·ªông v√πng crop v·ª´a ph·∫£i\n        x_pad = int(0.2 * bw)\n        y_pad = int(0.3 * bh)\n\n        x1 = max(0, x1 - x_pad)\n        y1 = max(0, y1 - y_pad)\n        x2 = min(w, x2 + x_pad)\n        y2 = min(h, y2 + y_pad)\n\n        crop = img[y1:y2, x1:x2]\n        cv2.imwrite(os.path.join(output_dir, file), crop)\n\n        print(f\"‚úîÔ∏è Cropped: {file}\")\n\nprint(\"‚úÖ DONE ALL FOLDERS\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X√≥a file output.zip n·∫øu ƒë√£ t·ªìn t·∫°i\n!rm /kaggle/working/output.zip\n\n# N√©n to√†n b·ªô th∆∞ m·ª•c output th√†nh file output.zip\n!zip -r /kaggle/working/output.zip /kaggle/working/dataset/base_image_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### T·∫°o promt cho ·∫£nh (ƒê√£ x·ª≠ l√Ω v√† l∆∞u trong repo)\nModel: BLIP","metadata":{}},{"cell_type":"code","source":"!pip install transformers accelerate timm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport random\n\n# Load BLIP model (ch·ªâ ƒë·ªÉ tham kh·∫£o, kh√¥ng d√πng t√™n ri√™ng)\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n\nroot_path = \"/kaggle/input/dataset-for-dynamicid/dataset/base_image_dataset\"\ndata_list = []\n\n# Danh s√°ch bi·ªÉu c·∫£m v√† pose ƒë·ªÉ sinh prompt m√¥ t·∫£ khu√¥n m·∫∑t\nexpressions = [\n    \"smiling\", \"neutral expression\", \"serious face\", \"surprised look\",\n    \"angry expression\", \"sad face\", \"laughing\", \"calm expression\",\n    \"thoughtful look\", \"eyes closed\", \"wide-eyed\", \"gentle smile\",\n    \"big grin\", \"pouting lips\", \"relaxed face\", \"focused eyes\"\n]\n\nposes = [\n    \"facing forward\", \"looking to the left\", \"looking to the right\",\n    \"head slightly tilted\", \"chin down\", \"chin up\", \"side profile\",\n    \"three-quarter view\", \"looking upward\", \"looking downward\"\n]\n\ndef generate_face_prompt():\n    expr = random.choice(expressions)\n    pose = random.choice(poses)\n    return f\"{expr}, {pose}\"\n\n# Duy·ªát qua t·ª´ng th∆∞ m·ª•c c√° nh√¢n (x = ID)\nfor person_id in sorted(os.listdir(root_path)):\n    person_path = os.path.join(root_path, person_id)\n    if not os.path.isdir(person_path):\n        continue\n\n    # L·∫•y danh s√°ch ·∫£nh trong th∆∞ m·ª•c\n    img_files = [f for f in sorted(os.listdir(person_path)) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n\n    # N·∫øu nhi·ªÅu h∆°n 35 ‚Üí ch·ªâ l·∫•y 35 ·∫£nh ƒë·∫ßu\n    img_files = img_files[:35]\n\n    # N·∫øu √≠t h∆°n 35 ‚Üí nh√¢n b·∫£n cho ƒë·ªß\n    while len(img_files) < 35:\n        img_files.append(img_files[-1])  # l·∫∑p l·∫°i ·∫£nh cu·ªëi c√πng\n\n    # T·∫°o ƒë√∫ng 35 entry cho m·ªói th∆∞ m·ª•c\n    for idx, img_file in enumerate(img_files):\n        img_path = os.path.join(person_path, img_file)\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading {img_path}: {e}\")\n            continue\n\n        # Caption b·∫±ng BLIP (ch·ªâ ƒë·ªÉ tham kh·∫£o, kh√¥ng d√πng t√™n ri√™ng)\n        try:\n            inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n            out = model.generate(**inputs, max_new_tokens=50)\n            caption_raw = processor.decode(out[0], skip_special_tokens=True)\n        except Exception as e:\n            print(f\"BLIP failed on {img_path}: {e}\")\n            caption_raw = \"\"\n\n        # Thay caption b·∫±ng prompt m√¥ t·∫£ khu√¥n m·∫∑t (kh√¥ng t√™n ri√™ng)\n        caption = generate_face_prompt()\n\n        rel_path = f\"{person_id}/{img_file}\"\n        data_list.append({\n            \"image_file\": rel_path,\n            \"text\": caption\n        })\n\n# Xu·∫•t file JSON\noutput_json = \"/kaggle/working/data.json\"\nwith open(output_json, \"w\") as f:\n    json.dump(data_list, f, indent=2)\n\nprint(\"Total entries created:\", len(data_list))\nprint(f\"Saved {len(data_list)} entries to {output_json}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# S·ª≠a code train_SAA.py (ƒê√£ s·ª≠a l·∫°i trong repo gihub => Kh√¥ng c·∫ßn ch·∫°y cell n√†y)\nfrom pathlib import Path\n\npath = Path(\"/kaggle/working/DynamicID/train_SAA.py\")\ntext = path.read_text()\n\ntext = text.replace(\n    \"ip_tokens = self.image_proj_model(image_embeds)\",\n    \"\"\"dummy_id = torch.zeros(\n        image_embeds.shape[0],\n        512,\n        device=image_embeds.device,\n        dtype=image_embeds.dtype\n    )\n    ip_tokens = self.image_proj_model(dummy_id, image_embeds)\"\"\"\n)\n\npath.write_text(text)\nprint(\"‚úÖ Fixed ProjPlusModel forward call\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# S·ª≠a code train_SAA.py (ƒê√£ s·ª≠a l·∫°i trong repo gihub => Kh√¥ng c·∫ßn ch·∫°y cell n√†y)\nfrom pathlib import Path\nimport re\n\npath = Path(\"/kaggle/working/DynamicID/train_SAA.py\")\ntext = path.read_text()\n\npattern = re.compile(r\"image_proj_model\\s*=\\s*ProjPlusModel\\([\\s\\S]*?\\)\", re.MULTILINE)\n\nreplacement = \"\"\"image_proj_model = ProjPlusModel(\n        cross_attention_dim=unet.config.cross_attention_dim,\n        id_embeddings_dim=512,\n        clip_embeddings_dim=image_encoder.config.hidden_size,\n        num_tokens=args.num_tokens,\n    )\"\"\"\n\ntext = pattern.sub(replacement, text)\npath.write_text(text)\n\nprint(\"‚úÖ Fixed ProjPlusModel init\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# S·ª≠a code train_SAA.py (ƒê√£ s·ª≠a l·∫°i trong repo gihub => Kh√¥ng c·∫ßn ch·∫°y cell n√†y)\nfrom pathlib import Path\n\nfile_path = Path(\"/kaggle/working/DynamicID/train_SAA.py\")\ntext = file_path.read_text()\n\nold = \"attn_procs[name] = SAProcessor()\"\n\nnew = \"\"\"attn_procs[name] = SAProcessor(\n                hidden_size=hidden_size,\n                rank=4,\n                network_alpha=4,\n            )\"\"\"\n\ntext = text.replace(old, new)\nfile_path.write_text(text)\n\nprint(\"‚úÖ Fixed SAProcessor init\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# S·ª≠a code train_SAA.py (ƒê√£ s·ª≠a l·∫°i trong repo gihub => Kh√¥ng c·∫ßn ch·∫°y cell n√†y)\nfrom pathlib import Path\n\nfile_path = Path(\"/kaggle/working/DynamicID/train_SAA.py\")\ntext = file_path.read_text()\n\ntext = text.replace(\n    \"attn_procs[name].load_state_dict(weights)\",\n    \"attn_procs[name].load_state_dict(weights, strict=False)\"\n)\n\nfile_path.write_text(text)\nprint(\"‚úÖ Fixed load_state_dict(strict=False)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}