{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14262584,"sourceType":"datasetVersion","datasetId":9101068},{"sourceId":14277176,"sourceType":"datasetVersion","datasetId":9111899},{"sourceId":14279700,"sourceType":"datasetVersion","datasetId":9029656},{"sourceId":14280824,"sourceType":"datasetVersion","datasetId":9112684}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BIGDATA FOR DYNAMICID","metadata":{}},{"cell_type":"markdown","source":"## Clone DynamicID Repository\n- G·ªëc: https://github.com/traananhdat/DynamicID\n- M·ªõi: https://github.com/LongTruongPhamHai/bigdata-for-dynamicid","metadata":{}},{"cell_type":"code","source":"# X√≥a th∆∞ m·ª•c c≈© n·∫øu ƒë√£ t·ªìn t·∫°i\n!rm -rf /kaggle/working/bigdata-for-dynamicid\n\n# Clone repo github\n!git clone https://github.com/LongTruongPhamHai/bigdata-for-dynamicid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:15:06.997053Z","iopub.execute_input":"2025-12-24T14:15:06.997774Z","iopub.status.idle":"2025-12-24T14:15:08.924901Z","shell.execute_reply.started":"2025-12-24T14:15:06.997745Z","shell.execute_reply":"2025-12-24T14:15:08.924253Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'bigdata-for-dynamicid'...\nremote: Enumerating objects: 453, done.\u001b[K\nremote: Counting objects: 100% (72/72), done.\u001b[K\nremote: Compressing objects: 100% (50/50), done.\u001b[K\nremote: Total 453 (delta 44), reused 50 (delta 22), pack-reused 381 (from 1)\u001b[K\nReceiving objects: 100% (453/453), 50.73 MiB | 49.38 MiB/s, done.\nResolving deltas: 100% (152/152), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Train SAA","metadata":{}},{"cell_type":"code","source":"# T·∫£i th∆∞ vi·ªán\n!pip uninstall -y diffusers huggingface_hub transformers accelerate peft\n\n!pip install \\\n  diffusers==0.27.2 \\\n  transformers==4.41.2 \\\n  accelerate==0.27.2 \\\n  peft==0.10.0 \\\n  huggingface_hub==0.23.4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ki·ªÉm tra th∆∞ vi·ªán\n!pip show diffusers transformers accelerate peft","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch \\\n  --num_processes 1 \\\n  /kaggle/working/bigdata-for-dynamicid/train_SAA.py \\\n  --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 \\\n  --image_encoder_path openai/clip-vit-large-patch14 \\\n  --data_root_path /kaggle/input/dataset-for-dynamicid/dataset/base_image_dataset \\\n  --data_json_file /kaggle/working/bigdata-for-dynamicid/data.json \\\n  --mixed_precision fp16 \\\n  --train_batch_size 1 \\\n  --num_train_epochs 10","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# N√©n file checkpoint\n!rm /kaggle/working/checkpoint-2000.zip\n# !zip -r /kaggle/working/checkpoint-2000.zip /kaggle/working/sd-ip_adapter/checkpoint-2000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X√≥a 3 file checkpoint kh√¥ng c·∫ßn thi·∫øt\n!rm /kaggle/working/sd-ip_adapter/checkpoint-2000/optimizer.bin \\\n    /kaggle/working/sd-ip_adapter/checkpoint-2000/random_states_0.pkl \\\n    /kaggle/working/sd-ip_adapter/checkpoint-2000/scaler.pt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train IMR","metadata":{}},{"cell_type":"markdown","source":"### T·∫°o SAA.bin t·ª´ /sd-ip_adapter/checkpoint-2000/model.safetensors","metadata":{}},{"cell_type":"code","source":"# Ki·ªÉm tra th∆∞ vi·ªán\n!pip show torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom safetensors.torch import load_file\n\nckpt_path = \"/kaggle/input/saa-model-safetensors/saa-model.safetensors\"\nout_path = \"/kaggle/working/models/SAA.bin\"\n\nckpt = load_file(ckpt_path)\n\nsaa_state = {}\n\nfor k, v in ckpt.items():\n    if \"lora\" not in k.lower():\n        continue\n\n    k = k.replace(\"unet.\", \"\")\n    k = k.replace(\".processor.\", \".\")\n    k = k.replace(\"transformer_blocks.0.\", \"\")\n\n    saa_state[k] = v.cpu()\n\nassert len(saa_state) > 0, \"‚ùå No SAA weights extracted\"\n\nos.makedirs(os.path.dirname(out_path), exist_ok=True)\n\ntorch.save(\n    {\n        \"state_dict\": saa_state,\n        \"rank\": 4,          \n        \"type\": \"SAA\"\n    },\n    out_path\n)\n\nprint(f\"‚úÖ Exported {len(saa_state)} SAA weights ‚Üí {out_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ch·∫°y file create_IMR_cache.py","metadata":{}},{"cell_type":"code","source":"# T·∫£i th∆∞ vi·ªán\n!pip install -U insightface onnxruntime-gpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nsaa = torch.load(\"/kaggle/working/models/SAA.bin\", map_location=\"cpu\")\n\nprint(saa.keys())\nprint(\"Rank:\", saa[\"rank\"])\nprint(\"Num params:\", len(saa[\"state_dict\"]))\n\nfor k, v in list(saa[\"state_dict\"].items())[:3]:\n    print(k, v.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python /kaggle/working/bigdata-for-dynamicid/create_IMR_cache.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X√≥a file IMR_cache.zip n·∫øu ƒë√£ t·ªìn t·∫°i\n!rm /kaggle/working/IMR_cache.zip\n\n# N√©n to√†n b·ªô th∆∞ m·ª•c cache th√†nh file IMR_cache.zip\n!zip -r /kaggle/working/IMR_cache.zip /kaggle/working/cache","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### T·∫°o promt v√† landmark cho ·∫£nh \nModel:\n- BLIP: T·∫°o promt\n- FaceMesh: T·∫°o landmark","metadata":{}},{"cell_type":"code","source":"!pip install dlib opencv-python-headless pillow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm /kaggle/working/shape_predictor_68_face_landmarks.dat.bz2\n!rm /kaggle/working/shape_predictor_68_face_landmarks.dat\n\n# t·∫£i file n√©n t·ª´ dlib.net\n!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n\n# gi·∫£i n√©n file .bz2 th√†nh .dat\n!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\n\n!rm /kaggle/working/shape_predictor_68_face_landmarks.dat.bz2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport random\nimport numpy as np\nfrom PIL import Image\nimport dlib\n\n# ================== PATH CONFIG ==================\nBASE_INPUT_DIR = \"/kaggle/input/dataset-for-dynamicid/dataset/base_image_dataset\"  # ·∫£nh g·ªëc ch·ªâ ƒë·ªçc\nOUTPUT_DIR = \"/kaggle/working/dataset/base_image_dataset\"  # n∆°i l∆∞u landmark + prompt\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# ================== SETTINGS ==================\nexpressions = [\n    \"sad face\", \"happy expression\", \"neutral expression\",\n    \"angry expression\", \"surprised look\", \"calm expression\", \"serious face\"\n]\nposes = [\n    \"facing forward\", \"looking to the left\", \"looking to the right\",\n    \"chin down\", \"chin up\", \"head slightly tilted\", \"three-quarter view\"\n]\n\nTARGET_SIZE = (512, 512)  # resize v·ªÅ 512x512\n\ndef generate_face_prompt():\n    return f\"{random.choice(expressions)}, {random.choice(poses)}\"\n\n# ================== INIT LANDMARK MODEL ==================\npredictor_path = \"/kaggle/working/shape_predictor_68_face_landmarks.dat\"\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(predictor_path)\n\n# ================== PROCESS ==================\nperson_ids = sorted([d for d in os.listdir(BASE_INPUT_DIR)\n                     if os.path.isdir(os.path.join(BASE_INPUT_DIR, d)) and d.isdigit()])\n\nfor pid in person_ids:\n    person_input_dir = os.path.join(BASE_INPUT_DIR, pid)\n    person_output_dir = os.path.join(OUTPUT_DIR, pid)\n    os.makedirs(person_output_dir, exist_ok=True)\n\n    img_files = [f for f in sorted(os.listdir(person_input_dir)) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n\n    # Gi·ªØ t·ªëi ƒëa 35 ·∫£nh / ng∆∞·ªùi, nh√¢n b·∫£n n·∫øu √≠t\n    img_files = img_files[:35]\n    while len(img_files) < 35:\n        img_files.append(img_files[-1])\n\n    for img_file in img_files:\n        img_path = os.path.join(person_input_dir, img_file)\n        img_id = os.path.splitext(img_file)[0]\n\n        try:\n            # --------- Load v√† resize ·∫£nh g·ªëc ---------\n            img_pil = Image.open(img_path).convert(\"RGB\")\n            img_resized = img_pil.resize(TARGET_SIZE, Image.BICUBIC)\n            img_output_path = os.path.join(person_output_dir, img_file)\n            img_resized.save(img_output_path)\n\n            # --------- Landmark v·ªõi dlib ---------\n            img_np = np.array(img_resized)\n            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n\n            faces = detector(gray)\n            if len(faces) > 0:\n                face = faces[0]  # ch·ªâ l·∫•y khu√¥n m·∫∑t ƒë·∫ßu ti√™n\n                shape = predictor(gray, face)\n                landmarks = [(pt.x, pt.y) for pt in shape.parts()]\n\n                # V·∫Ω landmark th√†nh ·∫£nh ƒëen tr·∫Øng\n                landmark_img = np.zeros((TARGET_SIZE[1], TARGET_SIZE[0]), dtype=np.uint8)\n                for (x, y) in landmarks:\n                    if 0 <= x < TARGET_SIZE[0] and 0 <= y < TARGET_SIZE[1]:\n                        landmark_img[y, x] = 255\n\n                landmark_save_path = os.path.join(person_output_dir, f\"{img_id}_landmark.png\")\n                cv2.imwrite(landmark_save_path, landmark_img)\n\n            # --------- Prompt ---------\n            prompt_text = generate_face_prompt()\n            txt_save_path = os.path.join(person_output_dir, f\"{img_id}.txt\")\n            with open(txt_save_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(prompt_text)\n\n        except Exception as e:\n            print(f\"[SKIP] {img_path}: {e}\")\n            continue\n\nprint(\"‚úÖ Ho√†n t·∫•t: landmarks (dlib 68 ƒëi·ªÉm), ·∫£nh 512x512 v√† prompt ƒë∆∞·ª£c l∆∞u trong\", OUTPUT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport random\nimport numpy as np\nfrom PIL import Image\nimport dlib\n\n# ================== PATH CONFIG ==================\nBASE_INPUT_DIR = \"/kaggle/input/dataset-for-dynamicid/dataset/base_image_dataset\"  # ·∫£nh g·ªëc ch·ªâ ƒë·ªçc\nOUTPUT_DIR = \"/kaggle/working/dataset/base_image_dataset\"  # n∆°i l∆∞u landmark + prompt\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# ================== SETTINGS ==================\nexpressions = [\n    \"sad face\", \"happy expression\", \"neutral expression\",\n    \"angry expression\", \"surprised look\", \"calm expression\", \"serious face\"\n]\nposes = [\n    \"facing forward\", \"looking to the left\", \"looking to the right\",\n    \"chin down\", \"chin up\", \"head slightly tilted\", \"three-quarter view\"\n]\n\nIMG_SIZE = (512, 512)       # ·∫£nh feed VAE/UNet\nLANDMARK_SIZE = (64, 64)    # landmark feed keypoint_encoder/IMR\n\ndef generate_face_prompt():\n    return f\"{random.choice(expressions)}, {random.choice(poses)}\"\n\n# ================== INIT LANDMARK MODEL ==================\npredictor_path = \"/kaggle/working/shape_predictor_68_face_landmarks.dat\"\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(predictor_path)\n\n# ================== PROCESS ==================\nperson_ids = sorted([d for d in os.listdir(BASE_INPUT_DIR)\n                     if os.path.isdir(os.path.join(BASE_INPUT_DIR, d)) and d.isdigit()])\n\nfor pid in person_ids:\n    person_input_dir = os.path.join(BASE_INPUT_DIR, pid)\n    person_output_dir = os.path.join(OUTPUT_DIR, pid)\n    os.makedirs(person_output_dir, exist_ok=True)\n\n    img_files = [f for f in sorted(os.listdir(person_input_dir)) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n\n    # Gi·ªØ t·ªëi ƒëa 35 ·∫£nh / ng∆∞·ªùi, nh√¢n b·∫£n n·∫øu √≠t\n    img_files = img_files[:35]\n    while len(img_files) < 35:\n        img_files.append(img_files[-1])\n\n    for img_file in img_files:\n        img_path = os.path.join(person_input_dir, img_file)\n        img_id = os.path.splitext(img_file)[0]\n\n        try:\n            # --------- Load v√† resize ·∫£nh g·ªëc (512x512) ---------\n            img_pil = Image.open(img_path).convert(\"RGB\")\n            img_resized = img_pil.resize(IMG_SIZE, Image.BICUBIC)\n            img_output_path = os.path.join(person_output_dir, img_file)\n            img_resized.save(img_output_path)\n\n            # --------- Landmark v·ªõi dlib ---------\n            img_np = np.array(img_resized)\n            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n            faces = detector(gray)\n\n            if len(faces) > 0:\n                face = faces[0]  # ch·ªâ l·∫•y khu√¥n m·∫∑t ƒë·∫ßu ti√™n\n                shape = predictor(gray, face)\n                landmarks = [(pt.x, pt.y) for pt in shape.parts()]\n\n                # V·∫Ω landmark th√†nh ·∫£nh ƒëen tr·∫Øng 512x512\n                landmark_img = np.zeros(IMG_SIZE[::-1], dtype=np.uint8)\n                for (x, y) in landmarks:\n                    if 0 <= x < IMG_SIZE[0] and 0 <= y < IMG_SIZE[1]:\n                        landmark_img[y, x] = 255\n\n                # Resize landmark v·ªÅ 64x64\n                landmark_img_small = cv2.resize(landmark_img, LANDMARK_SIZE, interpolation=cv2.INTER_NEAREST)\n                landmark_save_path = os.path.join(person_output_dir, f\"{img_id}_landmark.png\")\n                cv2.imwrite(landmark_save_path, landmark_img_small)\n\n            # --------- Prompt ---------\n            prompt_text = generate_face_prompt()\n            txt_save_path = os.path.join(person_output_dir, f\"{img_id}.txt\")\n            with open(txt_save_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(prompt_text)\n\n        except Exception as e:\n            print(f\"[SKIP] {img_path}: {e}\")\n            continue\n\nprint(\"‚úÖ Ho√†n t·∫•t: ·∫£nh 512x512, landmark 64x64 v√† prompt ƒë∆∞·ª£c l∆∞u trong\", OUTPUT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X√≥a file dataset.zip n·∫øu ƒë√£ t·ªìn t·∫°i\n!rm /kaggle/working/dataset.zip\n\n# N√©n to√†n b·ªô th∆∞ m·ª•c dataset th√†nh file dataset.zip\n!zip -r /kaggle/working/dataset.zip /kaggle/working/dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ch·∫°y file train_IMR.py","metadata":{}},{"cell_type":"code","source":"# T·∫°o th∆∞ m·ª•c l∆∞u model\n!mkdir -p /kaggle/working/sd-v1-5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from diffusers import StableDiffusionPipeline\nimport torch\n\n# T·∫£i model SD-v1-5 t·ª´ Hugging Face\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    dtype=torch.float16\n)\n\n# L∆∞u ra th∆∞ m·ª•c chu·∫©n diffusers\npipe.save_pretrained(\"/kaggle/working/sd-v1-5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch \ncheckpoint = torch.load(\"/kaggle/working/bigdata-for-dynamicid/models/SAA.bin\", map_location=\"cpu\")\nprint(checkpoint.keys())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python /kaggle/working/bigdata-for-dynamicid/train_IMR.py \\\n    --pretrained_model_name_or_path /kaggle/working/sd-v1-5 \\\n    --IMR_depth 1 \\\n    --num_tokens 16 \\\n    --resolution 512 \\\n    --save_steps 500 \\\n    --output_dir /kaggle/working/bigdata-for-dynamicid/output \\\n    --logging_dir logs \\\n    --learning_rate 1e-4 \\\n    --weight_decay 0.01 \\\n    --num_train_epochs 20 \\\n    --train_batch_size 4 \\\n    --dataloader_num_workers 2 \\\n    --mixed_precision fp16 \\\n    --report_to tensorboard","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T14:15:18.152792Z","iopub.execute_input":"2025-12-24T14:15:18.153587Z","iopub.status.idle":"2025-12-24T14:15:41.690911Z","shell.execute_reply.started":"2025-12-24T14:15:18.153554Z","shell.execute_reply":"2025-12-24T14:15:41.690038Z"}},"outputs":[{"name":"stdout","text":"2025-12-24 14:15:25.737587: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766585725.758995     483 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766585725.765578     483 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766585725.783138     483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766585725.783166     483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766585725.783169     483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766585725.783173     483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/diffusers/models/lora.py:208: FutureWarning: `LoRALinearLayer` is deprecated and will be removed in version 1.0.0. Use of `LoRALinearLayer` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n  deprecate(\"LoRALinearLayer\", \"1.0.0\", deprecation_message)\n/usr/local/lib/python3.12/dist-packages/diffusers/configuration_utils.py:141: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\nTraceback (most recent call last):\n  File \"/kaggle/working/bigdata-for-dynamicid/train_IMR.py\", line 639, in <module>\n    main()\n  File \"/kaggle/working/bigdata-for-dynamicid/train_IMR.py\", line 586, in main\n    pred_tokens = imr(\n                  ^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 819, in forward\n    return model_forward(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 807, in __call__\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/bigdata-for-dynamicid/model.py\", line 278, in forward\n    x = self.proj_in(x)\n        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1962, in __getattr__\n    raise AttributeError(\nAttributeError: 'IMR' object has no attribute 'proj_in'\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Cell c≈© - Ch·ªâ xem, kh√¥ng c·∫ßn ch·∫°y","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocess\n### Rename image (Ch·∫°y tr·ª±c ti·∫øp tr√™n m√°y)","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\n\ndef rename_images(directory: Path):\n    files = [f for f in directory.iterdir() if f.is_file()]\n    files.sort()  # s·∫Øp x·∫øp ƒë·ªÉ c√≥ th·ª© t·ª± ·ªïn ƒë·ªãnh\n\n    changed = 0\n    for idx, f in enumerate(files, start=1):\n        if idx > 35:\n            break\n\n        new_name = f\"{idx:04d}.jpg\"\n        new_path = f.with_name(new_name)\n\n        if new_path == f:\n            continue  # ƒë√£ ƒë√∫ng ƒë·ªãnh d·∫°ng\n        if new_path.exists():\n            print(f\"SKIP: T·ªìn t·∫°i s·∫µn {new_path.name}, b·ªè qua {f.name}\")\n            continue\n\n        f.rename(new_path)\n        changed += 1\n        print(f\"RENAMED: {f.name} -> {new_path.name}\")\n\n    print(f\"Done. ƒê√£ ƒë·ªïi t√™n {changed} file.\")\n\n\nif __name__ == \"__main__\":\n    dir_path = Path(\n        r\"D:\\DEV\\.Projects\\TLU\\PTDL\\DynamicID_fork_prv\\dataset\\base_image_dataset\\2\"\n    ).resolve()\n    rename_images(dir_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### FaceCrop Image (ƒê√£ x·ª≠ l√Ω v√† l∆∞u v√†o dataset-for-dynamicid)\nModel: MediaPipe Face Detection","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y numpy\n!pip install numpy==1.26.4 mediapipe==0.10.31 opencv-python","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X√≥a th∆∞ m·ª•c n·∫øu ƒë√£ t·ªìn t·∫°i\n!rm -rf /kaggle/working/mediapipe_models\n\n# T·∫°o th∆∞ m·ª•c m·ªõi\n!mkdir -p /kaggle/working/mediapipe_models\n\n# T·∫£i model short-range\n!wget \\\nhttps://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/latest/blaze_face_short_range.tflite \\\n-O /kaggle/working/mediapipe_models/blaze_face_short_range.tflite","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FaceCrop cho 1 th∆∞ m·ª•c con c·ªßa dataset\nimport cv2\nimport os\nfrom mediapipe import Image, ImageFormat\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\n\n# ===== PATH =====\nbase_input_dir = \"/kaggle/input/dataset-for-dynamicid/dataset/base_image_dataset\"\nbase_output_dir = \"/kaggle/working/dataset/base_image_dataset/6\"\nshort_model = \"/kaggle/working/mediapipe_models/blaze_face_short_range.tflite\"\n\n# ===== CH·ªåN TH∆Ø M·ª§C MU·ªêN X·ª¨ L√ù =====\ntarget_subfolder = \"6\"   # ƒë·ªïi t√™n th∆∞ m·ª•c b·∫°n mu·ªën crop ·ªü ƒë√¢y\n\ndef load_detector(model_path):\n    base_options = python.BaseOptions(model_asset_path=model_path)\n    options = vision.FaceDetectorOptions(\n        base_options=base_options,\n        running_mode=vision.RunningMode.IMAGE\n    )\n    return vision.FaceDetector.create_from_options(options)\n\n# ===== LOAD MODEL =====\ndetector = load_detector(short_model)\n\n# ===== PROCESS CH·ªà M·ªòT TH∆Ø M·ª§C =====\ninput_dir = os.path.join(base_input_dir, target_subfolder)\noutput_dir = os.path.join(base_output_dir, target_subfolder)\n\nif os.path.isdir(input_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"üîÑ Processing folder: {target_subfolder}\")\n\n    for file in os.listdir(input_dir):\n        if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n            continue\n\n        img = cv2.imread(os.path.join(input_dir, file))\n        if img is None:\n            continue\n\n        h, w, _ = img.shape\n        mp_image = Image(\n            image_format=ImageFormat.SRGB,\n            data=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        )\n\n        result = detector.detect(mp_image)\n\n        if not result.detections:\n            cv2.imwrite(os.path.join(output_dir, file), img)\n            print(f\"‚ö†Ô∏è No face detected, saved original: {file}\")\n            continue\n\n        bbox = result.detections[0].bounding_box\n        x1, y1 = int(bbox.origin_x), int(bbox.origin_y)\n        bw, bh = int(bbox.width), int(bbox.height)\n        x2, y2 = x1 + bw, y1 + bh\n\n        x_pad = int(0.2 * bw)\n        y_pad = int(0.3 * bh)\n\n        x1 = max(0, x1 - x_pad)\n        y1 = max(0, y1 - y_pad)\n        x2 = min(w, x2 + x_pad)\n        y2 = min(h, y2 + y_pad)\n\n        crop = img[y1:y2, x1:x2]\n        cv2.imwrite(os.path.join(output_dir, file), crop)\n\n        print(f\"‚úîÔ∏è Cropped: {file}\")\n\nprint(\"‚úÖ DONE ONE FOLDER\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FaceCrop to√†n b·ªô th∆∞ m·ª•c trong dataset\nimport cv2\nimport os\nfrom mediapipe import Image, ImageFormat\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\n\n# ===== PATH =====\nbase_input_dir = \"/kaggle/input/dataset-for-dynamicid/dataset/base_image_dataset\"\nbase_output_dir = \"/kaggle/working/dataset/base_image_dataset\"\nshort_model = \"/kaggle/working/mediapipe_models/blaze_face_short_range.tflite\"\n\ndef load_detector(model_path):\n    base_options = python.BaseOptions(model_asset_path=model_path)\n    options = vision.FaceDetectorOptions(\n        base_options=base_options,\n        running_mode=vision.RunningMode.IMAGE\n    )\n    return vision.FaceDetector.create_from_options(options)\n\n# ===== LOAD MODEL =====\ndetector = load_detector(short_model)\n\n# ===== PROCESS ALL SUBFOLDERS =====\nfor subfolder in sorted(os.listdir(base_input_dir)):\n    input_dir = os.path.join(base_input_dir, subfolder)\n    output_dir = os.path.join(base_output_dir, subfolder)\n\n    if not os.path.isdir(input_dir):\n        continue  # b·ªè qua n·∫øu kh√¥ng ph·∫£i th∆∞ m·ª•c\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    print(f\"üîÑ Processing folder: {subfolder}\")\n\n    for file in os.listdir(input_dir):\n        if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n            continue\n\n        img = cv2.imread(os.path.join(input_dir, file))\n        if img is None:\n            continue\n\n        h, w, _ = img.shape\n        mp_image = Image(\n            image_format=ImageFormat.SRGB,\n            data=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        )\n\n        result = detector.detect(mp_image)\n\n        if not result.detections:\n            # ‚ùå Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c m·∫∑t ‚Üí l∆∞u nguy√™n ·∫£nh\n            cv2.imwrite(os.path.join(output_dir, file), img)\n            print(f\"‚ö†Ô∏è No face detected, saved original: {file}\")\n            continue\n\n        # l·∫•y khu√¥n m·∫∑t ƒë·∫ßu ti√™n\n        bbox = result.detections[0].bounding_box\n        x1, y1 = int(bbox.origin_x), int(bbox.origin_y)\n        bw, bh = int(bbox.width), int(bbox.height)\n        x2, y2 = x1 + bw, y1 + bh\n\n        # m·ªü r·ªông v√πng crop v·ª´a ph·∫£i\n        x_pad = int(0.2 * bw)\n        y_pad = int(0.3 * bh)\n\n        x1 = max(0, x1 - x_pad)\n        y1 = max(0, y1 - y_pad)\n        x2 = min(w, x2 + x_pad)\n        y2 = min(h, y2 + y_pad)\n\n        crop = img[y1:y2, x1:x2]\n        cv2.imwrite(os.path.join(output_dir, file), crop)\n\n        print(f\"‚úîÔ∏è Cropped: {file}\")\n\nprint(\"‚úÖ DONE ALL FOLDERS\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X√≥a file output.zip n·∫øu ƒë√£ t·ªìn t·∫°i\n!rm /kaggle/working/output.zip\n\n# N√©n to√†n b·ªô th∆∞ m·ª•c output th√†nh file output.zip\n!zip -r /kaggle/working/output.zip /kaggle/working/dataset/base_image_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# S·ª≠a code train_SAA.py (ƒê√£ s·ª≠a l·∫°i trong repo gihub => Kh√¥ng c·∫ßn ch·∫°y cell n√†y)\nfrom pathlib import Path\n\npath = Path(\"/kaggle/working/DynamicID/train_SAA.py\")\ntext = path.read_text()\n\ntext = text.replace(\n    \"ip_tokens = self.image_proj_model(image_embeds)\",\n    \"\"\"dummy_id = torch.zeros(\n        image_embeds.shape[0],\n        512,\n        device=image_embeds.device,\n        dtype=image_embeds.dtype\n    )\n    ip_tokens = self.image_proj_model(dummy_id, image_embeds)\"\"\"\n)\n\npath.write_text(text)\nprint(\"‚úÖ Fixed ProjPlusModel forward call\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# S·ª≠a code train_SAA.py (ƒê√£ s·ª≠a l·∫°i trong repo gihub => Kh√¥ng c·∫ßn ch·∫°y cell n√†y)\nfrom pathlib import Path\nimport re\n\npath = Path(\"/kaggle/working/DynamicID/train_SAA.py\")\ntext = path.read_text()\n\npattern = re.compile(r\"image_proj_model\\s*=\\s*ProjPlusModel\\([\\s\\S]*?\\)\", re.MULTILINE)\n\nreplacement = \"\"\"image_proj_model = ProjPlusModel(\n        cross_attention_dim=unet.config.cross_attention_dim,\n        id_embeddings_dim=512,\n        clip_embeddings_dim=image_encoder.config.hidden_size,\n        num_tokens=args.num_tokens,\n    )\"\"\"\n\ntext = pattern.sub(replacement, text)\npath.write_text(text)\n\nprint(\"‚úÖ Fixed ProjPlusModel init\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# S·ª≠a code train_SAA.py (ƒê√£ s·ª≠a l·∫°i trong repo gihub => Kh√¥ng c·∫ßn ch·∫°y cell n√†y)\nfrom pathlib import Path\n\nfile_path = Path(\"/kaggle/working/DynamicID/train_SAA.py\")\ntext = file_path.read_text()\n\nold = \"attn_procs[name] = SAProcessor()\"\n\nnew = \"\"\"attn_procs[name] = SAProcessor(\n                hidden_size=hidden_size,\n                rank=4,\n                network_alpha=4,\n            )\"\"\"\n\ntext = text.replace(old, new)\nfile_path.write_text(text)\n\nprint(\"‚úÖ Fixed SAProcessor init\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# S·ª≠a code train_SAA.py (ƒê√£ s·ª≠a l·∫°i trong repo gihub => Kh√¥ng c·∫ßn ch·∫°y cell n√†y)\nfrom pathlib import Path\n\nfile_path = Path(\"/kaggle/working/DynamicID/train_SAA.py\")\ntext = file_path.read_text()\n\ntext = text.replace(\n    \"attn_procs[name].load_state_dict(weights)\",\n    \"attn_procs[name].load_state_dict(weights, strict=False)\"\n)\n\nfile_path.write_text(text)\nprint(\"‚úÖ Fixed load_state_dict(strict=False)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}